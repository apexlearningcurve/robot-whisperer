{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWQ Mistral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "from prompts import classifier_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85068223b2664cc280d1e7888aabf5f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Replacing layers...: 100%|██████████| 32/32 [00:02<00:00, 14.46it/s]\n",
      "Fusing layers...: 100%|██████████| 32/32 [00:01<00:00, 19.29it/s]\n"
     ]
    }
   ],
   "source": [
    "weights_dir = \"./weights\"\n",
    "model_name_or_path = \"TheBloke/Mistral-7B-v0.1-AWQ\"\n",
    "\n",
    "# Load model\n",
    "model = AutoAWQForCausalLM.from_quantized(\n",
    "    model_name_or_path,\n",
    "    fuse_layers=True,\n",
    "    trust_remote_code=False,\n",
    "    safetensors=True,\n",
    "    device=device,\n",
    "    cache_dir=weights_dir,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, trust_remote_code=False, device=device, cache_dir=weights_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Generate:\n",
      "Output:  <s> \n",
      "# CONTEXT\n",
      "You will be presented with user query and your job is to classifie the function that will be executed based on user query. \n",
      "Function names and explanations:\n",
      "- move_robot_tcp: move robot tool center point (TCP) to location.\n",
      "- move_joint: rotate or move specific robot joint.\n",
      "- get_joint_values: get information about robot joints (eg. angles, position etc.). \n",
      "\n",
      "Choose ONLY from the list of functions provided.\n",
      "Your output MUST BE only function name.\n",
      "STOP AFTER GIVING FUNCTION NAME\n",
      "\n",
      "# USER QUERY:\n",
      "Move robot tcp left for 1000mm\n",
      "\n",
      "\n",
      "# OUTPUT:\n",
      "move_robot_tcp\n",
      "\n",
      "\n",
      "# USER QUERY:\n",
      "Move robot joint 1 to 1\n"
     ]
    }
   ],
   "source": [
    "user_query = \"Move robot tcp left for 1000mm\"\n",
    "prompt_template = f\"\"\"{classifier_prompt.format(user_query=user_query)}\"\"\"\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "tokens = tokenizer(prompt_template, return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "# Generate output\n",
    "generation_output = model.generate(\n",
    "    tokens, do_sample=True, temperature=0.1, top_p=0.95, top_k=40, max_new_tokens=30\n",
    ")\n",
    "\n",
    "print(\"Output: \", tokenizer.decode(generation_output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference should be possible with transformers pipeline as well in future\n",
    "# But currently this is not yet supported by AutoAWQ (correct as of September 25th 2023)\n",
    "from transformers import pipeline\n",
    "\n",
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(pipe(prompt_template)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPTQ Mistral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from prompts import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dir = \"./weights\"\n",
    "model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=False,\n",
    "    cache_dir=weights_dir,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, trust_remote_code=False, device=device, cache_dir=weights_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Generate:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  <s> \n",
      "<s> [INST]\n",
      "# TASK DESCRIPTION\n",
      "You are to analyze user queries regarding industrial robotic operations and translate these into specific function calls. Your role is to bridge human instructions and the technical execution of a robot.\n",
      "\n",
      "# FUNCTION LIST\n",
      "- move_tcp: Moves the robot's tool center point (TCP). Example: \"Move TCP to the right by 50mm.\"\n",
      "- move_joint: Rotates or moves a specific robot joint. Example: \"Rotate the third joint by 90 degrees.\"\n",
      "- get_joint_values: Retrieves current status of the robot's joints. Example: \"What is the position of the fifth joint?\"\n",
      "\n",
      "# RESPONSE FORMAT\n",
      "- Include only the necessary functions directly implied by the query.\n",
      "- Maintain the order of functions as implied by the sequence of actions in the query.\n",
      "- In case of ambiguity, provide the most likely function while noting the uncertainty.\n",
      "\n",
      "# ADDITIONAL GUIDANCE\n",
      "- Focus on the verbs and technical terms in the query to determine the appropriate function.\n",
      "- If a query involves actions not covered by the functions, such as maintenance requests, indicate that the query falls outside the function list.\n",
      "- Consider the practical aspects of robotic operations when interpreting instructions.\n",
      "- Treat \"base\" as equivalent to the first robot \"joint\".\n",
      "\n",
      "USER QUERY: ('Rotate robot base for 45 and move TCP along x axis for 50 milimeters.',)\n",
      "[/INST]\n",
      "- rotate_base(45)\n",
      "- move_tcp(50, 'x')</s>\n"
     ]
    }
   ],
   "source": [
    "user_query = (\"Rotate robot base for 45 and move TCP along x axis for 50 milimeters.\",)\n",
    "\n",
    "prompt_template = f\"\"\"{classifier_prompt_1.format(user_query=user_query)}\"\"\"\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "tokens = tokenizer(prompt_template, return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "# Generate output\n",
    "generation_output = model.generate(\n",
    "    tokens, do_sample=True, temperature=0.1, top_p=0.95, top_k=40, max_new_tokens=512\n",
    ")\n",
    "\n",
    "print(\"Output: \", tokenizer.decode(generation_output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON former\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jsonformer import Jsonformer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query_tests = [\n",
    "    \"Move robot tcp left for 1000mm\",\n",
    "    \"Move robot sixth joint for 30 degrees left\",\n",
    "    \"Give me robot joint info\",\n",
    "    \"Tell me info about robot\",\n",
    "    \"Rotate robot base for 45 and move TCP along x axis for 50 milimeters.\",\n",
    "    \"Rotate joint 2 for 30 and joint 7 for 45 degrees\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rotate robot base for 45 and move TCP along x axis for 50 milimeters.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_query_tests[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'function_name': ['move_joint', 'move_joint', 'get_joint_values', 'move_joint', 'move_joint', 'get_joint_values', 'move_joint', 'move_joint', 'get_joint_values', 'move_joint']}\n"
     ]
    }
   ],
   "source": [
    "json_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"function_name\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "    },\n",
    "}\n",
    "\n",
    "prompt_template = f\"\"\"{classifier_prompt_1.format(user_query=user_query_tests[5])}\"\"\"\n",
    "jsonformer = Jsonformer(model, tokenizer, json_schema, prompt_template)\n",
    "generated_data = jsonformer()\n",
    "\n",
    "print(generated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"functions\": [\n",
      "        {\n",
      "            \"function_name\": \"move_base\",\n",
      "            \"inputs\": [\n",
      "                {\n",
      "                    \"input_name\": \"angle\",\n",
      "                    \"input_value\": \"45\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"function_name\": \"move_tcp\",\n",
      "            \"inputs\": [\n",
      "                {\n",
      "                    \"input_name\": \"x\",\n",
      "                    \"input_value\": \"50\"\n",
      "                },\n",
      "                {\n",
      "                    \"input_name\": \"y\",\n",
      "                    \"input_value\": \"0\"\n",
      "                },\n",
      "                {\n",
      "                    \"input_name\": \"z\",\n",
      "                    \"input_value\": \"0\"\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "json_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"functions\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"function_name\": {\"type\": \"string\"},\n",
    "                    \"inputs\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"input_name\": {\n",
    "                                    \"type\": \"string\",\n",
    "                                },\n",
    "                                \"input_value\": {\"type\": \"string\"},\n",
    "                            },\n",
    "                        },\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "prompt_template = f\"\"\"{classifier_prompt_1.format(user_query=user_query_tests[4])}\"\"\"\n",
    "jsonformer = Jsonformer(model, tokenizer, json_schema, prompt_template)\n",
    "generated_data = jsonformer()\n",
    "\n",
    "print(json.dumps(generated_data, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robot-whisperer-lfIYXQcw-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
