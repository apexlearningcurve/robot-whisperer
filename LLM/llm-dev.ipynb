{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWQ Mistral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "from prompts import classifier_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85068223b2664cc280d1e7888aabf5f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Replacing layers...: 100%|██████████| 32/32 [00:02<00:00, 14.46it/s]\n",
      "Fusing layers...: 100%|██████████| 32/32 [00:01<00:00, 19.29it/s]\n"
     ]
    }
   ],
   "source": [
    "weights_dir = \"./weights\"\n",
    "model_name_or_path = \"TheBloke/Mistral-7B-v0.1-AWQ\"\n",
    "\n",
    "# Load model\n",
    "model = AutoAWQForCausalLM.from_quantized(\n",
    "    model_name_or_path,\n",
    "    fuse_layers=True,\n",
    "    trust_remote_code=False,\n",
    "    safetensors=True,\n",
    "    device=device,\n",
    "    cache_dir=weights_dir,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, trust_remote_code=False, device=device, cache_dir=weights_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Generate:\n",
      "Output:  <s> \n",
      "# CONTEXT\n",
      "You will be presented with user query and your job is to classifie the function that will be executed based on user query. \n",
      "Function names and explanations:\n",
      "- move_robot_tcp: move robot tool center point (TCP) to location.\n",
      "- move_joint: rotate or move specific robot joint.\n",
      "- get_joint_values: get information about robot joints (eg. angles, position etc.). \n",
      "\n",
      "Choose ONLY from the list of functions provided.\n",
      "Your output MUST BE only function name.\n",
      "STOP AFTER GIVING FUNCTION NAME\n",
      "\n",
      "# USER QUERY:\n",
      "Move robot tcp left for 1000mm\n",
      "\n",
      "\n",
      "# OUTPUT:\n",
      "move_robot_tcp\n",
      "\n",
      "\n",
      "# USER QUERY:\n",
      "Move robot joint 1 to 1\n"
     ]
    }
   ],
   "source": [
    "user_query = \"Move robot tcp left for 1000mm\"\n",
    "prompt_template = f\"\"\"{classifier_prompt.format(user_query=user_query)}\"\"\"\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "tokens = tokenizer(prompt_template, return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "# Generate output\n",
    "generation_output = model.generate(\n",
    "    tokens, do_sample=True, temperature=0.1, top_p=0.95, top_k=40, max_new_tokens=30\n",
    ")\n",
    "\n",
    "print(\"Output: \", tokenizer.decode(generation_output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference should be possible with transformers pipeline as well in future\n",
    "# But currently this is not yet supported by AutoAWQ (correct as of September 25th 2023)\n",
    "from transformers import pipeline\n",
    "\n",
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(pipe(prompt_template)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPTQ Mistral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from prompts import classifier_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dir = \"./weights\"\n",
    "model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=False,\n",
    "    cache_dir=weights_dir,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, trust_remote_code=False, device=device, cache_dir=weights_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"Move robot tcp left for 1000mm\"\n",
    "prompt_template = f\"\"\"{classifier_prompt.format(user_query=user_query)}\"\"\"\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "tokens = tokenizer(prompt_template, return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "# Generate output\n",
    "generation_output = model.generate(\n",
    "    tokens, do_sample=True, temperature=0.1, top_p=0.95, top_k=40, max_new_tokens=512\n",
    ")\n",
    "\n",
    "print(\"Output: \", tokenizer.decode(generation_output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON former\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jsonformer import Jsonformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query_tests = [\n",
    "    \"Move robot tcp left for 1000mm\",\n",
    "    \"Move robot sixth joint for 30 degrees left\",\n",
    "    \"Give me robot joint info\",\n",
    "    \"Tell me info about robot\",\n",
    "    \"Rotate robot base for 45 and move TCP along x axis for 50 milimeters.\",\n",
    "    \"Rotate joint 2 for 30 and joint 7 for 45 degrees\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rotate robot base for 45 and move TCP along x axis for 50 milimeters.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_query_tests[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s/.cache/pypoetry/virtualenvs/robot-whisperer-lfIYXQcw-py3.10/lib/python3.10/site-packages/transformers/generation/utils.py:1518: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'function_name': ['move_joint', 'move_joint']}\n"
     ]
    }
   ],
   "source": [
    "json_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"function_name\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "    },\n",
    "}\n",
    "\n",
    "prompt_template = f\"\"\"{classifier_prompt.format(user_query=user_query_tests[5])}\"\"\"\n",
    "jsonformer = Jsonformer(model, tokenizer, json_schema, prompt_template)\n",
    "generated_data = jsonformer()\n",
    "\n",
    "print(generated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenizer.encode(classifier_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "616"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<s>[INST]\n",
      "# TASK DESCRIPTION\n",
      "Analyze user queries related to robotic operations and determine the necessary functions to execute these operations. Each query involves actions to be performed by a robotic system.\n",
      "\n",
      "# FUNCTION LIST\n",
      "- move_robot_tcp: Moves the robot's tool center point (TCP) to a specified location or along specified axis.\n",
      "- move_joint: Rotates or moves a specific robot joint/base/motor/actuator.\n",
      "- get_joint_values: Retrieves information about the robot's joints (e.g., angles, positions).\n",
      "\n",
      "# RESPONSE FORMAT\n",
      "- Answer in the form of a LIST of function names.\n",
      "- Only include function names from the provided list.\n",
      "- For a single function requirement, list only one. \n",
      "- For multiple functions, list all that apply.\n",
      "- A function may appear multiple times in the list if the query calls for its repeated use.\n",
      "- Preserve the sequence of functions as they appear in the query.\n",
      "- Do not add any function not explicitly required by the query.\n",
      "\n",
      "# ADDITIONAL RESTRICTIONS\n",
      "- Avoid assumptions: If the query lacks clear information, do not infer or guess.\n",
      "- Be precise: Ensure the functions match the specific actions requested in the query.\n",
      "- Ignore irrelevant details: Focus only on details that directly influence function choice.\n",
      "\n",
      "# GUIDANCE\n",
      "- Pay close attention to action verbs and technical terms in the query, as they often indicate required functions.\n",
      "- If a query involves movement or rotation not specified by 'TCP' or a 'joint', do not select any function.\n",
      "- In cases of compound queries (multiple actions), break down each action to decide the appropriate function.\n",
      "- \"base\" is same as first robot \"joint\".\n",
      "\n",
      "# EXAMPLES\n",
      "\n",
      "1. USER QUERY: \"Move the TCP 100 millimeters along the y-axis.\"\n",
      "   RESPONSE: [move_robot_tcp]\n",
      "\n",
      "2. USER QUERY: \"Move the TCP 73 millimeters along the negative y-axis, then rotate the last robot joint by 90 degrees.\"\n",
      "   RESPONSE: [move_robot_tcp, move_joint]\n",
      "\n",
      "3. USER QUERY: \"What are the current angles of the robot's joints?\"\n",
      "   RESPONSE: [get_joint_values]\n",
      "\n",
      "4. USER QUERY: \"Move robot base for -30 degrees then move fifth joint for 120\"\n",
      "   RESPONSE: [move_joint, move_joint]\n",
      "\n",
      "5. USER QUERY: {user_query}\n",
      "   RESPONSE: \n",
      "[/INST]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classifier_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robot-whisperer-lfIYXQcw-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
