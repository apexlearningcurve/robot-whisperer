{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWQ Mistral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "from prompts import classifier_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85068223b2664cc280d1e7888aabf5f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Replacing layers...: 100%|██████████| 32/32 [00:02<00:00, 14.46it/s]\n",
      "Fusing layers...: 100%|██████████| 32/32 [00:01<00:00, 19.29it/s]\n"
     ]
    }
   ],
   "source": [
    "weights_dir = \"./weights\"\n",
    "model_name_or_path = \"TheBloke/Mistral-7B-v0.1-AWQ\"\n",
    "\n",
    "# Load model\n",
    "model = AutoAWQForCausalLM.from_quantized(\n",
    "    model_name_or_path,\n",
    "    fuse_layers=True,\n",
    "    trust_remote_code=False,\n",
    "    safetensors=True,\n",
    "    device=device,\n",
    "    cache_dir=weights_dir,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, trust_remote_code=False, device=device, cache_dir=weights_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Generate:\n",
      "Output:  <s> \n",
      "# CONTEXT\n",
      "You will be presented with user query and your job is to classifie the function that will be executed based on user query. \n",
      "Function names and explanations:\n",
      "- move_robot_tcp: move robot tool center point (TCP) to location.\n",
      "- move_joint: rotate or move specific robot joint.\n",
      "- get_joint_values: get information about robot joints (eg. angles, position etc.). \n",
      "\n",
      "Choose ONLY from the list of functions provided.\n",
      "Your output MUST BE only function name.\n",
      "STOP AFTER GIVING FUNCTION NAME\n",
      "\n",
      "# USER QUERY:\n",
      "Move robot tcp left for 1000mm\n",
      "\n",
      "\n",
      "# OUTPUT:\n",
      "move_robot_tcp\n",
      "\n",
      "\n",
      "# USER QUERY:\n",
      "Move robot joint 1 to 1\n"
     ]
    }
   ],
   "source": [
    "user_query = \"Move robot tcp left for 1000mm\"\n",
    "prompt_template = f\"\"\"{classifier_prompt.format(user_query=user_query)}\"\"\"\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "tokens = tokenizer(prompt_template, return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "# Generate output\n",
    "generation_output = model.generate(\n",
    "    tokens, do_sample=True, temperature=0.1, top_p=0.95, top_k=40, max_new_tokens=30\n",
    ")\n",
    "\n",
    "print(\"Output: \", tokenizer.decode(generation_output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference should be possible with transformers pipeline as well in future\n",
    "# But currently this is not yet supported by AutoAWQ (correct as of September 25th 2023)\n",
    "from transformers import pipeline\n",
    "\n",
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(pipe(prompt_template)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPTQ Mistral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dir = \"./weights\"\n",
    "model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=False,\n",
    "    cache_dir=weights_dir,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, trust_remote_code=False, device=device, cache_dir=weights_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"Move robot tcp left for 1000mm\"\n",
    "prompt_template = f\"\"\"{classifier_prompt.format(user_query=user_query)}\"\"\"\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "tokens = tokenizer(prompt_template, return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "# Generate output\n",
    "generation_output = model.generate(\n",
    "    tokens, do_sample=True, temperature=0.1, top_p=0.95, top_k=40, max_new_tokens=512\n",
    ")\n",
    "\n",
    "print(\"Output: \", tokenizer.decode(generation_output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON former\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jsonformer import Jsonformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query_tests = [\"Move robot tcp left for 1000mm\", \"Move robot sixth joint for 30 degrees left\", \"Give me robot joint info\", \"Tell me info about robot\", \"Rotate robot joint for 45 and move TCP along x axis for 50 milimeters.\", \"Rotate joint 2 for 30 and joint 7 for 45 degrees\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rotate joint 2 for 30 and joint 7 for 45 degrees'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_query_tests[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'function_name': ['move_joint', 'move_joint']}\n"
     ]
    }
   ],
   "source": [
    "json_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"function_name\": {\n",
    "            \"type\": \"array\", \n",
    "            \"items\": {\"type\": \"string\"}},\n",
    "    },\n",
    "}\n",
    "\n",
    "prompt_template = f\"\"\"{classifier_prompt.format(user_query=user_query_tests[5])}\"\"\"\n",
    "jsonformer = Jsonformer(model, tokenizer, json_schema, prompt_template)\n",
    "generated_data = jsonformer()\n",
    "\n",
    "print(generated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robot-whisperer-lfIYXQcw-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
